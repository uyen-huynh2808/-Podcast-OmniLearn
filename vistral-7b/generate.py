from transformers import AutoTokenizer, AutoModelForCausalLM

HF_TOKEN = "your token "   # âš ï¸ thay báº±ng token cÃ¡ nhÃ¢n

model_id = "Viet-Mistral/Vistral-7B-Chat"

# Load tokenizer + model vá»›i token
tokenizer = AutoTokenizer.from_pretrained(model_id, use_auth_token=HF_TOKEN)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map="auto",
    # load_in_4bit=True,
    trust_remote_code=True,
    use_auth_token=HF_TOKEN
)

prompt = """
Báº¡n lÃ  má»™t giáº£ng viÃªn cÃ³ nhiá»‡m vá»¥ giÃºp há»c viÃªn tiáº¿p thu nhanh hÆ¡n.
HÃ£y chuyá»ƒn ná»™i dung Ä‘oáº¡n vÄƒn sau thÃ nh má»™t ká»‹ch báº£n podcast ngáº¯n,
theo dáº¡ng há»™i thoáº¡i tá»± nhiÃªn giá»¯a hai ngÆ°á»i (A vÃ  B).

YÃªu cáº§u:
- Giá»¯ nguyÃªn kiáº¿n thá»©c cá»‘t lÃµi, khÃ´ng thÃªm thÃ´ng tin sai lá»‡ch.
- Giá»ng Ä‘iá»‡u gáº§n gÅ©i, trÃ² chuyá»‡n nhÆ° báº¡n bÃ¨.
- Chia nhá» Ã½ thÃ nh Ä‘á»‘i thoáº¡i qua láº¡i, giÃºp ngÆ°á»i nghe dá»… nhá»›.
- CÃ³ vÃ­ dá»¥ minh há»a náº¿u phÃ¹ há»£p.

Äoáº¡n vÄƒn:
"Äá»c sÃ¡ch má»—i ngÃ y khÃ´ng chá»‰ giÃºp má»Ÿ rá»™ng vá»‘n tá»« vá»±ng mÃ  cÃ²n rÃ¨n luyá»‡n trÃ­ nhá»›.
CÃ¡c nghiÃªn cá»©u cho tháº¥y thÃ³i quen Ä‘á»c sÃ¡ch 20 phÃºt má»—i ngÃ y cÃ³ thá»ƒ cáº£i thiá»‡n kháº£ nÄƒng táº­p trung,
giáº£m cÄƒng tháº³ng vÃ  tÄƒng kháº£ nÄƒng tháº¥u hiá»ƒu ngÆ°á»i khÃ¡c."
"""

messages = [{"role": "user", "content": prompt}]

inputs = tokenizer.apply_chat_template(
    messages,
    add_generation_prompt=True,
    tokenize=True,
    return_dict=True,
    return_tensors="pt"
).to(model.device)

outputs = model.generate(
    **inputs,
    max_new_tokens=400,
    temperature=0.7,
    top_p=0.9,
    repetition_penalty=1.1
)

print("ğŸ™ï¸ Podcast Script:\n")
print(tokenizer.decode(outputs[0][inputs["input_ids"].shape[-1]:], skip_special_tokens=True))
